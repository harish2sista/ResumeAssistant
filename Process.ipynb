{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4b09256-8097-4588-938f-37a2f16507c4",
   "metadata": {},
   "source": [
    "# Workbench for ResumeAssistant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79634a30-eea3-403d-935a-a66ce9bd5dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdf2image import convert_from_path\n",
    "# ImageWorker takes in the path to jpg resume image and returns the contents of the resume in a json format. \n",
    "from resumeassistant.operator.worker import ImageWorker\n",
    "from resumeassistant.\n",
    "from openai import OpenAI\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad03abd-9b24-4e84-924b-07b7bd8f27ff",
   "metadata": {},
   "source": [
    "# PDF to jpg image converter function\n",
    "This function takes the input path for pdf-resume and output path for resume image. The output images are stored in 'jpg' format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9bffade2-11d4-42ee-bd17-d24ce4ec87d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_img(pdf_path, img_path):\n",
    "      # Method for converting pdf resumes to jpg images. \n",
    "      images = convert_from_path(pdf_path)\n",
    "      # print(type(images))\n",
    "      for i in range(len(images)):\n",
    "        \n",
    "            # Save pages as images in the pdf\n",
    "          images[i].save(img_path + str(i) +'.jpg', 'JPEG')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e22a3b50-9801-47d7-89c1-108bc0e3c9a5",
   "metadata": {},
   "source": [
    "This is the code to extract OpenAI key stored in `gpt_key.json` file. \\\n",
    "Format: \\\n",
    "    ```{ \n",
    "\t\"api_key\": \\<open_ai_key\\>\n",
    "    }```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35ffad11-5857-4851-bad4-e63cb22a1ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('gpt_key.json', 'r') as file:\n",
    "      data = json.load(file)\n",
    "      api_key = data[\"api_key\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4459786a-75de-4155-b004-ac40c32c1b23",
   "metadata": {},
   "source": [
    "## Resume Parsing\n",
    "This is the code for converting the PDF resume to an image and parsing it using \"GPT-4o\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2858039-6d9f-4c40-b436-ec9e47dc1ecd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'Harish Sista', 'contact': {'phone': '(201) 310-5683', 'email': 'hsista@stevens.edu', 'address': '2642 28th St, Apt 4F, Astoria, NY-11102'}, 'summary': 'Ph.D. Candidate researching in ML and NLP with 7+ years of experience working with ML models, Big Data, and Cloud Computing platforms and 2+ years of experience working in a professional environment. Spearheaded an Evidence Extraction and Fact Verification model using Prompt Engineering and tested it with various LLMs. Invented a Black Box Optimization model for online hyperparameter fine-tuning, significantly enhancing model performance in real-time applications.', 'skills': {'programming_languages': ['Python', 'C++', 'Java', 'Objective C', 'Swift', 'SQL'], 'ai_tools': ['PyTorch', 'OpenAI', 'HuggingFace', 'Transformers', 'TensorFlow', 'SciKitLearn', 'NLTK', 'NumPy'], 'llms': ['GPT', 'LLAMA', 'MISTRAL', 'BLOOM', 'BERT-based models'], 'developer_apis': ['Twitter', 'Facebook', 'YouTube', 'Google', 'Amazon'], 'cloud_platforms': ['GIT', 'PyPA', 'AWS', 'Google Cloud', 'Apple Store', 'Heroku'], 'research_interests': ['NLP', 'Prompt Engineering', 'Explainable Attention Networks', 'Generative Models']}, 'education': [{'degree': 'Ph.D. in Machine Learning', 'institution': 'Stevens Institute of Technology, Hoboken', 'duration': '01/2018 - Present'}, {'degree': 'M.Eng. in Software Engineering', 'institution': 'Stevens Institute of Technology, Hoboken', 'duration': '01/2014 - 12/2015'}, {'degree': 'B.Eng. in Electronics and Communication Engineering', 'institution': 'M.V.S.R Engineering College, Hyderabad, TG, India', 'duration': '08/2009 - 05/2013'}], 'research_experience': {'position': 'Infinity Lab, Stevens Institute of Technology', 'location': 'Hoboken, NJ', 'duration': '01/2018 - Present', 'responsibilities': ['Work with various LLMs to address the challenges of Fact Verification using Prompt Engineering, Text Generation, Classification and Evidence Extraction Tasks.', 'Invented a Black Box Optimization model for Online Hyperparameter finetuning.', 'Built various Python Libraries using Multiprocessing and GPU programming to work efficiently with big datasets.', 'Surveyed and implemented various Attention Networks, DNNs and Generative Models to perform NLP tasks of Data Extraction, Modeling, Regression, Ranking, and Classification.', 'Collaborated in creating various Scientific, social media, and Misinformation Datasets.', 'Developed an AI based iOS application to perform MoCA test for early detection of Alzheimerâ€™s.']}, 'work_experience': [{'position': 'Research Assistant', 'institution': 'Stevens Institute of Technology', 'location': 'Hoboken, NJ', 'duration': '10/2018 - 09/2019', 'responsibilities': ['Created a chemistry literature dataset for the purpose of similarity search with search queries.', 'Preprocessed the dataset, using PDF conversion tools and NLTK library to remove the noise.', 'Produced a Novel Generative Ranking Model using Topic Modeling and KeyPhrase analysis, to extract most relevant papers related to the search query.']}, {'position': 'Engineer/iOS Developer', 'institution': 'Fresh Digital Group', 'location': 'NY, NY', 'duration': '07/2016 - 06/2017', 'responsibilities': ['Collaborated with Content Writers and Creative Team to individually design and deploy ~30 chat applications for Amazon-Alexa which won appreciation from tech newsletters.', 'Developed various iOS, iMessage and tOS applications in conjunction with several teams.', 'Helped company obtain a preferred partnership with Amazon-Alexa and launching partnership with Microsoft-Cortana.']}]}\n"
     ]
    }
   ],
   "source": [
    "client = OpenAI(api_key=api_key)\n",
    "get_img('Harish_Resume_PhD.pdf', './Harish_Resume')\n",
    "\n",
    "sys_prompt = \"You are a resume parser. Given the image of the resume, parse every section and provide the output in a JSON format.\"\n",
    "user_prompt=\"Can you parse all sections of this resume?\"\n",
    "\n",
    "image_worker = ImageWorker(client=client, model_id=\"gpt-4o-mini\", sys_prompt=sys_prompt, user_prompt=user_prompt)\n",
    "parsed_resume = image_worker.get_output('Harish_Resume0.jpg')\n",
    "print(parsed_resume)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7061118-f685-4836-8cd0-646c24949a76",
   "metadata": {},
   "source": [
    "## Add CV and other information\n",
    "The current_cv instance keeps a record of the most recent updated Cover Letter and additional information on work experience, projects, achievements, and certifications. \n",
    "\n",
    "### Input arguments:\n",
    "`resume`=Parsed resume (Dict). \\\n",
    "`cv` = Cover letter text. \\\n",
    "`add_skills` = Additional skills not mentioned in the resume (Dict). \\\n",
    "`add_work_experience` = Additional work experience not mentioned in the resume (Dict). \\\n",
    "`add_education` = Additional education not mentioned in the resume (Dict). \\\n",
    "`add_projects` = Additional Projects not mentioned in the resume (Dict). \\\n",
    "`add_achievements` = Additional achievements not mentioned in the resume (Dict). \\\n",
    "`add_certifications` = Additional certifications not mentioned in the resume (Dict). \n",
    "\n",
    "#### additional_information format:\n",
    "```{``` \\\n",
    "```\"information\": text,``` \\\n",
    "```\"organization\": text,``` \\\n",
    "```\"date\": \"dd/mm/yyyy - dd/mm/yyyy\" (or) \"dd/mm/yyyy\"``` \\\n",
    "```}```\n",
    "\n",
    "The job_listing instance keeps a record of the requirements and description of the job posting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "29a6c5a8-649f-4d08-b625-1a680a9ec18b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['name', 'contact', 'summary', 'skills', 'education', 'research_experience', 'work_experience'])\n"
     ]
    }
   ],
   "source": [
    "print(parsed_resume.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1962296-a716-41bb-9b5a-480f16577eb8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py3.10] *",
   "language": "python",
   "name": "conda-env-py3.10-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
